{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.special as sp\n",
    "\n",
    "from gamma_def import *\n",
    "from gamma_def_rejection import *\n",
    "from gamma_def_score import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta = 0.75\n",
    "B = 4\n",
    "\n",
    "if (B > 1):\n",
    "    correction = True\n",
    "else:\n",
    "    correction = False\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "# Setup sizes\n",
    "K = np.array([100,40,15])\n",
    "D = 64*64\n",
    "N = 320\n",
    "n_latent = N*np.sum(K)+K[0]*D+np.sum(K[1:]*K[:-1])\n",
    "sigma = 0.1\n",
    "\n",
    "# Load data\n",
    "x = np.loadtxt('data/faces_training.csv',delimiter=',')\n",
    "alphaz = 0.1\n",
    "\n",
    "# Define truncation functions and stepsize updates\n",
    "trunc_shape = np.log(np.exp(1e-3)-1.)\n",
    "trunc_mean = np.log(np.exp(1e-4)-1.)\n",
    "\n",
    "def stepSize(iteration,sPrev,gradient,eta=1.0):\n",
    "    sCur = 0.1*(gradient**2) + 0.9*sPrev\n",
    "    step = eta*np.power(iteration,-0.5+1e-16)/(1.+np.sqrt(sCur))\n",
    "\n",
    "    return step,sCur\n",
    "\n",
    "def truncate_params(params):\n",
    "    ind = params[:,0] < trunc_shape\n",
    "    params[ind,0] = trunc_shape\n",
    "    ind = params[:,1] < trunc_mean\n",
    "    params[ind,1] = trunc_mean\n",
    "    return params\n",
    "\n",
    "# Initialize\n",
    "num_seed = 123\n",
    "npr.seed(num_seed)\n",
    "params_R = np.zeros((n_latent,2))\n",
    "steps = np.ones((n_latent,2))\n",
    "sCur_R = np.zeros((n_latent,2))\n",
    "ELBO_R = np.zeros(n_iter)\n",
    "\n",
    "params_R[:,0] = 0.5+sigma*npr.normal(size=n_latent)\n",
    "params_R[:,1] = sigma*npr.normal(size=n_latent)\n",
    "\n",
    "transformVar = np.log(1.+np.exp(params_R))\n",
    "ELBO_R[0] = estimate_elbo(transformVar[:,0],transformVar[:,1],K,x,alphaz)\n",
    "\n",
    "for n in range(1,n_iter):\n",
    "    print(n)\n",
    "    sGrad = reparam_gradient(transformVar[:,0],transformVar[:,1],x,K,alphaz,corr=correction,B=B)/(1.+np.exp(-params_R))\n",
    "    steps,sCur_R = stepSize(n+1,sCur_R,sGrad,eta)\n",
    "\n",
    "    params_R  = truncate_params(params_R+steps*sGrad)\n",
    "    transformVar = np.log(1.+np.exp(params_R))\n",
    "    ELBO_R[n] = estimate_elbo(transformVar[:,0],transformVar[:,1],K,x,alphaz)\n",
    "    if np.mod(n,100) == 0:\n",
    "        filename = 'results/Olivette_Eta'+str(eta)+'_B'+str(B)+'_corr'+str(correction)+'_ELBO.npy'\n",
    "        np.save(filename, ELBO_R[:n_iter])\n",
    "        filename = 'results/Olivette_Eta'+str(eta)+'_B'+str(B)+'_corr'+str(correction)+'_K1_'+str(K[0])+'_K2_'+str(K[1])+'_K3_'+str(K[2])+'_params_R.npy'\n",
    "        np.save(filename,np.log(1.+np.exp(params_R)))\n",
    "\n",
    "filename = 'results/Olivette_Eta'+str(eta)+'_B'+str(B)+'_corr'+str(correction)+'_ELBO.npy'\n",
    "np.save(filename, ELBO_R[:n_iter])\n",
    "filename = 'results/Olivette_Eta'+str(eta)+'_B'+str(B)+'_corr'+str(correction)+'_K1_'+str(K[0])+'_K2_'+str(K[1])+'_K3_'+str(K[2])+'_params_R.npy'\n",
    "np.save(filename,np.log(1.+np.exp(params_R)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
